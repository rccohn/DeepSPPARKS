# sample parameters

mlflow: # relevant mlflow parameters
  # path to this project's directory
  project_uri: git@github.com:rccohn/DeepSpparks.git#experiments/2022-04-14-GAT
  project_version: main
  experiment_name: TEST # tracking experiment name

dataset:
  name: candidate-grains-repeat20-toy-lmd-v1.0.0  # path relative to dataset root

  subgraph_radius: 4  # number of neighborhood shells of candidate grain to include in graph
  # how cgr values from multiple runs are combined into single output for classification
  # for datasets with a single run, using values like 'mean', 'max', etc will return the original value
  repeat_aggregator: mean

  # recompute features/targets, even if existing processed data is found
  force_process: False

  # which grains are used for training and evaluation
  # current valid options are "candidate_grain" and "all_grains"
  mask: candidate_grain


encoder:
  mode: test # "pca" or "autoencoder" or "test"
  # uri to pca or autoencoder model for node and edge features (not needed if mode == "test")
  node_feature_model_uri: ~
  edge_feature_model_uri: ~
  # number of pca components to extract for node and edge features, only needed if mode == "pca"
  node_n_components_max: 50
  edge_n_components_max: 50

  # number of pca components to use during training
  node_n_components: [50]
  edge_n_components: [50]

  # indices to extract node patches from
  node_patch_bounds: [80, 80, 176, 176]


entry: main  # if set to "eval", will run "evaluation.py.main()" instead of "sgc_combined.py.main()"

# model params
model:
  dropout1: 0.5 # dropout in GATConv
  dropout2: 0.5 # dropout between message passing layers
  heads: [8, 4, 2] # number of self-attention heads on

# cgr values below this -> normal grain growth. Otherwise, abnormal grain growth
cgr_thresh: [6, 8] #[6, 8, 10, 12, 14]

# settings for Adam
optimizer:
  lr: [0.005, 0.01] #[0.001, 0.005, 0.01] # learning rate
  decay: [0.0005, 0.01] #[0.001, 0.0005, 0.0001] # adam weight decay

# for nn training
training:
  max_epoch: 50
  checkpoint_epoch: 10 # model weights/metrics saved at each checkpoint

# dataloader (batch size)
loader:
  num_workers: 4
  batch_size:
    train: 3
    val: 3
    test: 3


# run id of best performing run to generate confusion matrices from
eval_run_id: null

device: cpu # can be 'cpu' or 'cuda'
