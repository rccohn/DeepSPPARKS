# environment variables for mlflow experiments

# Sample environment variables. Default values give hints and descriptions for what
# the values should look like, but need to be changed to real values (ie path to dataset,
# real host/port of tracking uri, etc) before running

# URI of mlflow tracking server
MLFLOW_TRACKING_URI=http://127.0.0.1:5000

# directory containing python and mlflow executibles used to parse params and execute 
# "mlflow run." Note this is just used to run the projec and is not the same environment 
# that the project itself runs in (ie it does not need ML libraries other than mlflow)
# To be compatible with any type of environment (venv, conda, etc), we point to the
# directory containing the python binary itself and not just the root of the environment
PYTHON_ENV=/home/USER/DeepSPPARKS/docker/worker-run-experiment/.mlflow_env/bin/

# ssh folder configured for container use (ie permissions/owner must be configured 
# for user "root", see readme for more info.) It is needed if mlflow tracking uses a
# sftp artifact backend.
# If you are not using an sftp artifact store, point CONTAINER_SSH to an existing
# directory (can be an empty directory) so that docker does not create an unused
# volume. Mounted as read only so container cannot accidentally alter ssh keys.
# for more info, see DeepSPPARK/docker/tracking_server_with_sftp/
CONTAINER_SSH=/home/USER/.ssh

# root directory for datasets (contains each dataset as a subdirectory)
# mounted as read-only so experiment can never accidentally alter raw data
RAW_DATA_ROOT=/home/USER/data/datasets

# processed data root, contains featurized data that can actually be used by a model during experiments
# mounted as read/write, allowing container to save featurized data to host, for re-use with later
# experiments with the same dataset/features/targets.
PROCESSED_DATA_ROOT=/home/USER/data/processed

# for experiments that use transfer learning with VGG16
VGG16_IMAGENET_PATH=/home/USER/models/vgg16.h5


