#  environment variables used in compose file are stored in .env
# docker-compose looks for .env by default, or you can
# use docker-compose --env-file <environment file> --file <docker-compose file>

version: "3.8" 
services:
  gcloud-sql-auth-proxy:  # connect to gcp backend
    image: ${SQL_AUTH_IMAGE}  # image containing sql proxy
    # for now, host networking (container shares networking with host machine)
    # is needed. eventually I might be able to figure out how to get this 
    # working without exposing all ports (using expose or ports keywords)
    network_mode: "host" 
    # NOTE: on a gcp instance already authenticated with a service account
    # credentials are not needed. In this case, you can remove the 
    # lines to mount and reference the credential file in the cloud sql 
    # auth proxy
    volumes:  # mount service account authorization key to container
     - type: bind
       source: ${SERVICE_ACCOUNT_KEY} # path to account json credentials
       target: /config
       read_only: true

    entrypoint: # command to run gcp sql proxy
     - /cloud_sql_proxy
     - -enable_iam_login
     - -credential_file=/config
     # note that postgres default port is 5432. If your host machine has 
     # postgres already running, 5432 will already be occupied,
     # so instead we use 5433
     # gcp sql instance connection name, address/port on host
     - -instances=${INSTANCE_CONNECTION}=tcp:0.0.0.0:${PPORT}
    
  mlflow-server:
    depends_on:  # need to start sql auth proxy first, otherwise
    # mlflow server will error out after not being able to connect to db
      - gcloud-sql-auth-proxy
    image: ${MLFLOW_IMAGE} # image containing mlflow, google-storage
    network_mode: "host" # again, eventually figure out how to remove host
    volumes: # mount service key to container
     - type: bind
       source: ${SERVICE_ACCOUNT_KEY}
       target: /config
       read_only: true
    environment:  # needed to use SA key for authorization
      - GOOGLE_APPLICATION_CREDENTIALS=/config
    entrypoint: # command to run mlflow ui or server
      - mlflow 
      - ${MLFLOW_OPTION}  # "ui" for veiwing results or "server" for tracking experiments
      - --host=0.0.0.0
      - --port=${MLFLOW_PORT}
      - --backend-store-uri
      - ${MLFLOW_BACKEND_URI} # postgres+psycopg2://<user>:<password>@host(127.0.0.1:5433)/db-name
      - --default-artifact-root
      - ${MLFLOW_ARTIFACT_ROOT} # gs://<bucket>/path/to/artifacts

  local-jupyter-server:
    # data exploration version includes jupyter
    image: rccohn/deepspparks-worker:dex
    # init processes needed to avoid the PID 1 zombie reaping issue
    init: true
    # expose port on host to access jupyter
    network_mode: "host"
    # mount host directory so jupyter can access them, and
    # ensure that new files are persisted when container is stopped
    volumes:
     - type: bind
       source: ${HOST_MOUNT_JUPYTER}
       target: /home/root/host-files
    
    # enable gpu
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    # path to start jupyter in
    environment:
      - "CONTAINER_NOTEBOOK_DIR"
      - "JUPYTER_PORT_HOST"
    # start jupyter server
    entrypoint:
      - "bash"
      - "start_jupyter.sh"
    
  

# TODO later
#  - add service account key as secret
#  - remove host networking, only expose needed ports on host machine
#  - alternatively, network ml server container to cloud sql auth container directly
# 
#secrets:
#  sa_key:
#    file: ./sa_key.json

#    secrets:
#      - source: sa_key
#        target: /sa_key.json

