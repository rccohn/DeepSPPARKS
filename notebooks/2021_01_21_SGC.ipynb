{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from src.graphs import Graph\n",
    "\n",
    "from src.utils import batcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SGConv\n",
    "\n",
    "\n",
    "class SGNet(torch.nn.Module):\n",
    "    def __init__(self, data, K=1, num_classes=None):\n",
    "        super().__init__()\n",
    "        if num_classes == None:\n",
    "            num_classes = 2\n",
    "        self.conv = SGConv(in_channels=data.num_features, out_channels=num_classes, K=K, cached=False)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = self.conv(data.x, data.edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, mask):\n",
    "    \"\"\"\n",
    "    Single iteration of training\n",
    "    \"\"\"\n",
    "    # set training mode to True (enabling dropout, etc)\n",
    "    model.train()\n",
    "    \n",
    "#     # make sure format of weights is correct\n",
    "#     model.double()\n",
    "    \n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # get output of model, which is log-probability (log of softmax)\n",
    "    # note mask is not applied because message passing needs all nodes\n",
    "    log_softmax = model(data)\n",
    "    \n",
    "    labels = data.y # labels of each node\n",
    "    \n",
    "    # apply training mask\n",
    "    nll_loss = F.nll_loss(log_softmax[mask], labels[mask])\n",
    "    \n",
    "    # backprop- compute gradients\n",
    "    nll_loss.backward()\n",
    "    \n",
    "    # backprop- update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "def compute_accuracy(model, data, mask):\n",
    "    # set eval mode to True (disable dropout, etc)\n",
    "    model.eval()\n",
    "    \n",
    "    #model.double()\n",
    "    \n",
    "    # get output of model\n",
    "    log_softmax = model(data)\n",
    "    \n",
    "    # get index of max value from softmax, equivalent to y pred\n",
    "    yp = log_softmax[mask].argmax(dim=1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return yp == data.y[mask]\n",
    "\n",
    "# run without gradient (faster)\n",
    "@torch.no_grad() \n",
    "def test(model, data):\n",
    "    return compute_accuracy(model, data, data.mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine label using criteria for AGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_agg(g):\n",
    "    if g.graph_attr['candidate_growth_ratio'] > 10 and g.graph_attr['candidate_rgr'] > 2.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to normalize features for neural network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(d, mask=None):\n",
    "    if mask == None:\n",
    "        mask = np.ones(len(d.x), np.bool)\n",
    "    d.x = (d.x - d.x[mask].mean(dim=0))/d.x[mask].std(dim=0)\n",
    "    d.edge_attr = (d.edge_attr - d.edge_attr.mean(dim=0)/d.edge_attr.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Path('..','data','candidate-grains-processed')\n",
    "data.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_all = [list(x.glob('*.json')) for x in data.glob('*') if x.is_dir() and len(list(x.glob('*.json'))) > 500] \n",
    "temp = []\n",
    "[temp.extend(r) for r in runs_all]\n",
    "runs_all = sorted(temp)\n",
    "rs = np.random.RandomState(seed=3346665170)\n",
    "rs.shuffle(runs_all)\n",
    "from multiprocessing import get_context, Pool\n",
    "def load_wrapper(x):\n",
    "    from src.graphs import Graph\n",
    "    g = Graph.from_json(x)\n",
    "    d = g.to_pyg_dataset()\n",
    "    y = np.zeros(len(g.nodes), np.int)\n",
    "    y[d.mask] = int(detect_agg(g))\n",
    "    d.y = torch.tensor(y, dtype=torch.long)\n",
    "    return d\n",
    "\n",
    "#with Pool(processes=8) as p:\n",
    "#    datasets_large = p.map(load_wrapper, runs_all[:1000])\n",
    "\n",
    "datasets_large = list(map(load_wrapper, runs_all[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine graphs into batches \n",
    "Combine individual runs into disconnected graphs containing sets of 100 runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batcher(datasets_large, batch_size=100, min_size=30)\n",
    "batches = [torch_geometric.data.Batch().from_data_list(b) for b in batches]\n",
    "for b in batches:\n",
    "    normalize_features(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to be reported during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_per_node(model, batches):\n",
    "    # total number of test nodes per batch\n",
    "    n_test_nodes = torch.tensor([b.mask.sum() for b in batches])\n",
    "    # average loss per test node per batch\n",
    "    avg_loss_batch = torch.tensor([F.nll_loss(model(b)[b.mask], b.y[b.mask]).detach() for b in batches])\n",
    "    \n",
    "    # total loss\n",
    "    total_loss = (n_test_nodes * avg_loss_batch).sum()\n",
    "    \n",
    "    # avg loss per node\n",
    "    avg_loss = total_loss / n_test_nodes.sum()\n",
    "    return avg_loss\n",
    "\n",
    "def mean_acc(model, batches):\n",
    "    # tensor of predictions on test nodes in each batch, concatenated into single array\n",
    "    predictions = torch.cat([test(model, b) for b in batches], dim=0)\n",
    "    acc = predictions.sum()/len(predictions) # total number correct (True) vs total number (all)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgn = SGNet(batches[0], K=3)\n",
    "sgn.double() # needed to prevent pytorch errors during training\n",
    "model = sgn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train acc: 0.4867 Val acc: 0.5200, Train Loss: 0.6990\n",
      "Epoch: 010, Train acc: 0.4967 Val acc: 0.5700, Train Loss: 0.6923\n",
      "Epoch: 015, Train acc: 0.5589 Val acc: 0.5600, Train Loss: 0.6864\n",
      "Epoch: 020, Train acc: 0.5822 Val acc: 0.5800, Train Loss: 0.6811\n",
      "Epoch: 025, Train acc: 0.5978 Val acc: 0.6200, Train Loss: 0.6765\n",
      "Epoch: 030, Train acc: 0.5944 Val acc: 0.6000, Train Loss: 0.6724\n",
      "Epoch: 035, Train acc: 0.6011 Val acc: 0.6100, Train Loss: 0.6687\n",
      "Epoch: 040, Train acc: 0.6022 Val acc: 0.6000, Train Loss: 0.6656\n",
      "Epoch: 045, Train acc: 0.5944 Val acc: 0.6000, Train Loss: 0.6628\n",
      "Epoch: 050, Train acc: 0.6022 Val acc: 0.6200, Train Loss: 0.6604\n",
      "Epoch: 055, Train acc: 0.5967 Val acc: 0.6300, Train Loss: 0.6583\n",
      "Epoch: 060, Train acc: 0.6011 Val acc: 0.6300, Train Loss: 0.6564\n",
      "Epoch: 065, Train acc: 0.6033 Val acc: 0.6300, Train Loss: 0.6548\n",
      "Epoch: 070, Train acc: 0.6033 Val acc: 0.6200, Train Loss: 0.6535\n",
      "Epoch: 075, Train acc: 0.6033 Val acc: 0.6200, Train Loss: 0.6523\n",
      "Epoch: 080, Train acc: 0.6022 Val acc: 0.6200, Train Loss: 0.6513\n",
      "Epoch: 085, Train acc: 0.6022 Val acc: 0.6200, Train Loss: 0.6505\n",
      "Epoch: 090, Train acc: 0.6056 Val acc: 0.6300, Train Loss: 0.6497\n",
      "Epoch: 095, Train acc: 0.6078 Val acc: 0.6300, Train Loss: 0.6491\n",
      "Epoch: 100, Train acc: 0.6067 Val acc: 0.6300, Train Loss: 0.6486\n",
      "Epoch: 105, Train acc: 0.6078 Val acc: 0.6300, Train Loss: 0.6482\n",
      "Epoch: 110, Train acc: 0.6122 Val acc: 0.6300, Train Loss: 0.6478\n",
      "Epoch: 115, Train acc: 0.6122 Val acc: 0.6300, Train Loss: 0.6475\n",
      "Epoch: 120, Train acc: 0.6133 Val acc: 0.6300, Train Loss: 0.6473\n",
      "Epoch: 125, Train acc: 0.6156 Val acc: 0.6300, Train Loss: 0.6471\n",
      "Epoch: 130, Train acc: 0.6156 Val acc: 0.6300, Train Loss: 0.6469\n",
      "Epoch: 135, Train acc: 0.6144 Val acc: 0.6200, Train Loss: 0.6468\n",
      "Epoch: 140, Train acc: 0.6167 Val acc: 0.6300, Train Loss: 0.6466\n",
      "Epoch: 145, Train acc: 0.6178 Val acc: 0.6300, Train Loss: 0.6465\n",
      "Epoch: 150, Train acc: 0.6178 Val acc: 0.6300, Train Loss: 0.6465\n",
      "Epoch: 155, Train acc: 0.6178 Val acc: 0.6300, Train Loss: 0.6464\n",
      "Epoch: 160, Train acc: 0.6178 Val acc: 0.6300, Train Loss: 0.6464\n",
      "Epoch: 165, Train acc: 0.6167 Val acc: 0.6300, Train Loss: 0.6463\n",
      "Epoch: 170, Train acc: 0.6156 Val acc: 0.6300, Train Loss: 0.6463\n",
      "Epoch: 175, Train acc: 0.6156 Val acc: 0.6300, Train Loss: 0.6463\n",
      "Epoch: 180, Train acc: 0.6144 Val acc: 0.6300, Train Loss: 0.6462\n",
      "Epoch: 185, Train acc: 0.6144 Val acc: 0.6300, Train Loss: 0.6462\n",
      "Epoch: 190, Train acc: 0.6144 Val acc: 0.6300, Train Loss: 0.6462\n",
      "Epoch: 195, Train acc: 0.6144 Val acc: 0.6300, Train Loss: 0.6462\n",
      "Epoch: 200, Train acc: 0.6144 Val acc: 0.6300, Train Loss: 0.6462\n"
     ]
    }
   ],
   "source": [
    "#optimizer = torch.optim.Adam(gat.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "log = 'Epoch: {:03d}, Train acc: {:.4f} Val acc: {:.4f}, Train Loss: {:.4f}'#', Val: {:.4f}'\n",
    "for epoch in range(1, 201):\n",
    "    for batch in batches[:-1]:\n",
    "        train(model, batch, optimizer, batch.mask)\n",
    "    if epoch % 5 == 0:\n",
    "        \n",
    "        tr_accs = mean_acc(model, batches[:-1])\n",
    "        val_accs = mean_acc(model, batches[-1:])        \n",
    "        losses = loss_per_node(model, batches)\n",
    "        \n",
    "        print(log.format(epoch, tr_accs, val_accs, losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidate predictions from all graphs and report confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "[[340 137]\n",
      " [210 213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66       477\n",
      "           1       0.61      0.50      0.55       423\n",
      "\n",
      "    accuracy                           0.61       900\n",
      "   macro avg       0.61      0.61      0.61       900\n",
      "weighted avg       0.61      0.61      0.61       900\n",
      "\n",
      "Validation\n",
      "[[37 16]\n",
      " [21 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.70      0.67        53\n",
      "           1       0.62      0.55      0.58        47\n",
      "\n",
      "    accuracy                           0.63       100\n",
      "   macro avg       0.63      0.63      0.63       100\n",
      "weighted avg       0.63      0.63      0.63       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gt = []\n",
    "pred = []\n",
    "for b in batches[:-1]:\n",
    "    gt.extend(b.y[b.mask].numpy().tolist())\n",
    "    pred.extend(model(b)[b.mask].argmax(1).numpy().tolist())\n",
    "cm_tr = confusion_matrix(gt, pred)\n",
    "\n",
    "gtv = []\n",
    "predv = []\n",
    "for b in batches[-1:]:\n",
    "    gtv.extend(b.y[b.mask].numpy().tolist())\n",
    "    predv.extend(model(b)[b.mask].argmax(1).numpy().tolist())\n",
    "cm_val = confusion_matrix(gtv, predv)\n",
    "\n",
    "print('Train')\n",
    "print(cm_tr)\n",
    "print(classification_report(gt, pred))\n",
    "print('Validation')\n",
    "print(cm_val)\n",
    "print(classification_report(gtv, predv))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
