{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from src.graphs import Graph\n",
    "from src.utils import batcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GATNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GATConv, SGConv\n",
    "\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self, data, heads_layer1, \n",
    "               heads_layer2, dropout, dropout_alphas, num_classes=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        num_features = data.num_features\n",
    "        if num_classes == None:\n",
    "            num_classes = 2  # hardcoded for now\n",
    "\n",
    "        self.conv1 = GATConv(in_channels=num_features, out_channels=8,\n",
    "                             heads=heads_layer1, concat=True, negative_slope=0.2, \n",
    "                             dropout=dropout_alphas)\n",
    "\n",
    "        self.conv2 = GATConv(in_channels=8*heads_layer1, out_channels=num_classes, \n",
    "                             heads=heads_layer2, concat=False, negative_slope=0.2,\n",
    "                             dropout=dropout_alphas)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x=data.x\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv1(x, data.edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, data.edge_index)\n",
    "      \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    \n",
    "class SGNet(torch.nn.Module):\n",
    "    def __init__(self, data, K=1, num_classes=None):\n",
    "        super().__init__()\n",
    "        if num_classes == None:\n",
    "            num_classes = 2\n",
    "        self.conv = SGConv(in_channels=data.num_features, out_channels=num_classes, K=K, cached=False)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x = self.conv(data.x, data.edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, mask):\n",
    "    \"\"\"\n",
    "    Single iteration of training\n",
    "    \"\"\"\n",
    "    # set training mode to True (enabling dropout, etc)\n",
    "    model.train()\n",
    "    \n",
    "#     # make sure format of weights is correct\n",
    "#     model.double()\n",
    "    \n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # get output of model, which is log-probability (log of softmax)\n",
    "    # note mask is not applied because message passing needs all nodes\n",
    "    log_softmax = model(data)\n",
    "    \n",
    "    labels = data.y # labels of each node\n",
    "    \n",
    "    # apply training mask\n",
    "    nll_loss = F.nll_loss(log_softmax[mask], labels[mask])\n",
    "    \n",
    "    # backprop- compute gradients\n",
    "    nll_loss.backward()\n",
    "    \n",
    "    # backprop- update parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "\n",
    "def compute_accuracy(model, data, mask):\n",
    "    # set eval mode to True (disable dropout, etc)\n",
    "    model.eval()\n",
    "    \n",
    "    #model.double()\n",
    "    \n",
    "    # get output of model\n",
    "    log_softmax = model(data)\n",
    "    \n",
    "    # get index of max value from softmax, equivalent to y pred\n",
    "    yp = log_softmax[mask].argmax(dim=1) \n",
    "    \n",
    "    \n",
    "    \n",
    "    return yp == data.y[mask]\n",
    "\n",
    "# run without gradient (faster)\n",
    "@torch.no_grad() \n",
    "def test(model, data):\n",
    "    return compute_accuracy(model, data, data.mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_agg(g):\n",
    "    if g.graph_attr['candidate_growth_ratio'] > 10 and g.graph_attr['candidate_rgr'] > 2.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Path('..','data','candidate-grains-processed')\n",
    "data.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_paths = list(sorted(data.glob('*'))[-1].glob('*.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(d, mask=None):\n",
    "    if mask == None:\n",
    "        mask = np.ones(len(d.x), np.bool)\n",
    "    d.x = (d.x - d.x[mask].mean(dim=0))/d.x[mask].std(dim=0)\n",
    "    d.edge_attr = (d.edge_attr - d.edge_attr.mean(dim=0)/d.edge_attr.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [Graph.from_json(x) for x in json_paths[:200]]\n",
    "datasets = [g.to_pyg_dataset() for g in graphs]\n",
    "for g, d in zip(graphs, datasets):\n",
    "    y = np.zeros(len(g.nodes), np.int)\n",
    "    y[d.mask] = int(detect_agg(g))\n",
    "    d.y = torch.tensor(y, dtype=torch.long)\n",
    "    normalize_features(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch_geometric.data.Batch().from_data_list(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train: 0.6200, Loss: 0.9178\n",
      "Epoch: 010, Train: 0.6300, Loss: 0.9527\n",
      "Epoch: 015, Train: 0.6300, Loss: 0.9713\n",
      "Epoch: 020, Train: 0.6300, Loss: 0.9840\n",
      "Epoch: 025, Train: 0.6300, Loss: 0.9966\n",
      "Epoch: 030, Train: 0.6300, Loss: 1.0082\n",
      "Epoch: 035, Train: 0.6300, Loss: 1.0196\n",
      "Epoch: 040, Train: 0.6300, Loss: 1.0304\n",
      "Epoch: 045, Train: 0.6300, Loss: 1.0396\n",
      "Epoch: 050, Train: 0.6300, Loss: 1.0475\n"
     ]
    }
   ],
   "source": [
    "gat = GATNet(datasets[0], 4, 4, 0.5, 0.5)\n",
    "gat.double()\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.Adam(gat.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "optimizer = torch.optim.Adam(gat.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "log = 'Epoch: {:03d}, Train: {:.4f}, Loss: {:.4f}'#', Val: {:.4f}'\n",
    "for epoch in range(1, 51):\n",
    "    train(gat, batch, optimizer, batch.mask)\n",
    "    #for d in datasets:\n",
    "    #    train(gat, d, optimizer, d.mask)\n",
    "    if epoch % 5 == 0:\n",
    "        tests = [test(gat, d) for d in datasets]\n",
    "        losses = [F.nll_loss(gat(d)[d.mask], d.y[d.mask]).detach().numpy() for d in datasets]\n",
    "        \n",
    "        print(log.format(epoch, np.mean(tests), np.mean(losses)), )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_all = [list(x.glob('*.json')) for x in data.glob('*') if x.is_dir() and len(list(x.glob('*.json'))) > 500] \n",
    "temp = []\n",
    "[temp.extend(r) for r in runs_all]\n",
    "runs_all = sorted(temp)\n",
    "rs = np.random.RandomState(seed=3346665170)\n",
    "rs.shuffle(runs_all)\n",
    "from multiprocessing import get_context, Pool\n",
    "def load_wrapper(x):\n",
    "    from src.graphs import Graph\n",
    "    g = Graph.from_json(x)\n",
    "    d = g.to_pyg_dataset()\n",
    "    y = np.zeros(len(g.nodes), np.int)\n",
    "    y[d.mask] = int(detect_agg(g))\n",
    "    d.y = torch.tensor(y, dtype=torch.long)\n",
    "    return d\n",
    "\n",
    "#with Pool(processes=8) as p:\n",
    "#    datasets_large = p.map(load_wrapper, runs_all[:1000])\n",
    "\n",
    "datasets_large = list(map(load_wrapper, runs_all[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batcher(datasets_large, batch_size=100, min_size=30)\n",
    "batches = [torch_geometric.data.Batch().from_data_list(b) for b in batches]\n",
    "for b in batches:\n",
    "    normalize_features(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_per_node(model, batches):\n",
    "    # total number of test nodes per batch\n",
    "    n_test_nodes = torch.tensor([b.mask.sum() for b in batches])\n",
    "    # average loss per test node per batch\n",
    "    avg_loss_batch = torch.tensor([F.nll_loss(model(b)[b.mask], b.y[b.mask]).detach() for b in batches])\n",
    "    \n",
    "    # total loss\n",
    "    total_loss = (n_test_nodes * avg_loss_batch).sum()\n",
    "    \n",
    "    # avg loss per node\n",
    "    avg_loss = total_loss / n_test_nodes.sum()\n",
    "    return avg_loss\n",
    "\n",
    "def mean_acc(model, batches):\n",
    "    # tensor of predictions on test nodes in each batch, concatenated into single array\n",
    "    predictions = torch.cat([test(model, b) for b in batches], dim=0)\n",
    "    acc = predictions.sum()/len(predictions) # total number correct (True) vs total number (all)\n",
    "    return acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GATNet(datasets[0], 4, 4, 0.5, 0.5)\n",
    "gat.double()\n",
    "sgn = SGNet(datasets[0], K=3)\n",
    "sgn.double()\n",
    "model = sgn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Train acc: 0.5056 Val acc: 0.5900, Train Loss: 0.6974\n",
      "Epoch: 010, Train acc: 0.5211 Val acc: 0.5500, Train Loss: 0.6901\n",
      "Epoch: 015, Train acc: 0.5344 Val acc: 0.5600, Train Loss: 0.6839\n",
      "Epoch: 020, Train acc: 0.5478 Val acc: 0.5500, Train Loss: 0.6787\n",
      "Epoch: 025, Train acc: 0.5644 Val acc: 0.5800, Train Loss: 0.6741\n",
      "Epoch: 030, Train acc: 0.5867 Val acc: 0.5900, Train Loss: 0.6702\n",
      "Epoch: 035, Train acc: 0.5889 Val acc: 0.5900, Train Loss: 0.6668\n",
      "Epoch: 040, Train acc: 0.6000 Val acc: 0.5600, Train Loss: 0.6638\n",
      "Epoch: 045, Train acc: 0.6022 Val acc: 0.5700, Train Loss: 0.6613\n",
      "Epoch: 050, Train acc: 0.6033 Val acc: 0.5900, Train Loss: 0.6590\n",
      "Epoch: 055, Train acc: 0.5989 Val acc: 0.6200, Train Loss: 0.6571\n",
      "Epoch: 060, Train acc: 0.6011 Val acc: 0.6400, Train Loss: 0.6554\n",
      "Epoch: 065, Train acc: 0.6056 Val acc: 0.6300, Train Loss: 0.6540\n",
      "Epoch: 070, Train acc: 0.6033 Val acc: 0.6300, Train Loss: 0.6527\n",
      "Epoch: 075, Train acc: 0.6044 Val acc: 0.6200, Train Loss: 0.6517\n",
      "Epoch: 080, Train acc: 0.6078 Val acc: 0.6300, Train Loss: 0.6508\n",
      "Epoch: 085, Train acc: 0.6078 Val acc: 0.6300, Train Loss: 0.6500\n",
      "Epoch: 090, Train acc: 0.6089 Val acc: 0.6300, Train Loss: 0.6493\n",
      "Epoch: 095, Train acc: 0.6111 Val acc: 0.6300, Train Loss: 0.6488\n",
      "Epoch: 100, Train acc: 0.6156 Val acc: 0.6300, Train Loss: 0.6483\n",
      "Epoch: 105, Train acc: 0.6156 Val acc: 0.6300, Train Loss: 0.6479\n",
      "Epoch: 110, Train acc: 0.6144 Val acc: 0.6300, Train Loss: 0.6476\n",
      "Epoch: 115, Train acc: 0.6167 Val acc: 0.6300, Train Loss: 0.6474\n",
      "Epoch: 120, Train acc: 0.6167 Val acc: 0.6300, Train Loss: 0.6471\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA NOT NORMALIZED IN BATCHES\n",
    "# TODO FIGURE OUT NORMALIZATION (pre or post mask????)\n",
    "\n",
    "#optimizer = torch.optim.Adam(gat.parameters(), lr=0.005, weight_decay=1e-3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "log = 'Epoch: {:03d}, Train acc: {:.4f} Val acc: {:.4f}, Train Loss: {:.4f}'#', Val: {:.4f}'\n",
    "for epoch in range(1, 201):\n",
    "    for batch in batches[:-1]:\n",
    "        train(model, batch, optimizer, batch.mask)\n",
    "    if epoch % 5 == 0:\n",
    "        \n",
    "        tr_accs = mean_acc(model, batches[:-1])\n",
    "        val_accs = mean_acc(model, batches[-1:])        \n",
    "        losses = loss_per_node(model, batches)\n",
    "        \n",
    "        print(log.format(epoch, tr_accs, val_accs, losses))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[378, 152],\n",
       "       [230, 240]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = []\n",
    "pred = []\n",
    "for b in batches[:-1]:\n",
    "    gt.extend(b.y[b.mask].numpy().tolist())\n",
    "    pred.extend(model(b)[b.mask].argmax(1).numpy().tolist())\n",
    "cm_tr = confusion_matrix(gt, pred)\n",
    "\n",
    "gt = []\n",
    "pred = []\n",
    "for b in batches[-1:]:\n",
    "    gt.extend(b.y[b.mask].numpy().tolist())\n",
    "    pred.extend(model(b)[b.mask].argmax(1).numpy().tolist())\n",
    "cm_val = confusion_matrix(gt, pred)\n",
    "\n",
    "print('Train')\n",
    "print(cm_tr)\n",
    "print('Validation')\n",
    "print(cm_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt.sum()/len(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.618"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.trace()/cm.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = Planetoid(root = './tmp', name='Cora', transform=T.NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = cora[0]\n",
    "cdata.mask = cdata.train_mask\n",
    "model_cora = GATNet(cdata, 4, 4, 0.5, 0.5, num_classes=len(cdata.y.unique()))\n",
    "\n",
    "model_cora.float()\n",
    "optimizer = torch.optim.Adam(model_cora.parameters(), lr=0.003, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Train: 0.2214, Loss: 1.9446\n",
      "Epoch: 020, Train: 0.9357, Loss: 1.9157\n",
      "Epoch: 040, Train: 0.9571, Loss: 1.8526\n",
      "Epoch: 060, Train: 0.9643, Loss: 1.7353\n",
      "Epoch: 080, Train: 0.9643, Loss: 1.5617\n",
      "Epoch: 100, Train: 0.9786, Loss: 1.3530\n",
      "Epoch: 120, Train: 0.9643, Loss: 1.1390\n",
      "Epoch: 140, Train: 0.9714, Loss: 0.9539\n",
      "Epoch: 160, Train: 0.9857, Loss: 0.8146\n",
      "Epoch: 180, Train: 0.9929, Loss: 0.7160\n",
      "Epoch: 200, Train: 0.9857, Loss: 0.6443\n"
     ]
    }
   ],
   "source": [
    "log = 'Epoch: {:03d}, Train: {:.4f}, Loss: {:.4f}'#', Val: {:.4f}'\n",
    "\n",
    "for epoch in range(0, 201):\n",
    "    train(model_cora, cdata, optimizer, cdata.train_mask)\n",
    "    if epoch % 20  == 0:\n",
    "        accs = mean_acc(model_cora, [cdata,])\n",
    "        losses = loss_per_node(model_cora, [cdata,])\n",
    "        print(log.format(epoch, accs, losses))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
